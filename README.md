# Awesome-Triton-Kernels
Collection of kernels written in Triton language (didn't seem to be a lot till now). Welcoming contribution!

[Main Repo by OpenAI](https://github.com/openai/triton)

[Official Tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html)

[Awesome resources from cuda-mode](https://github.com/cuda-mode/resource-stream), their [guide](https://www.youtube.com/watch?v=DdTsX6DQk24&ab_channel=CUDAMODE) to Triton

[Triton Kernel collection by cuda-mode](https://github.com/cuda-mode/triton-index)

[Puzzles by Sasha Rush](https://github.com/srush/Triton-Puzzles)

## General Operators
[attorch](https://github.com/BobMcDear/attorch) subset of PyTorch's nn module

[FlagGems](https://github.com/FlagOpen/FlagGems)

[Kernels by PyTorch Labs](https://github.com/pytorch-labs/applied-ai)

[scattermoe: Sparse Mixture-of-Experts](https://github.com/shawntan/scattermoe)

[Fused moe from vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py)

## Transformer
[Liger Kernel: Efficient Triton Kernels for LLM Training](https://github.com/linkedin/Liger-Kernel)

[Flash Linear Attention](https://github.com/sustcsonglin/flash-linear-attention)

[FLASHNN for LLM Serving](https://github.com/AlibabaPAI/FLASHNN)

[Kernels by Kernl](https://github.com/ELS-RD/kernl)

[Kernels by Unsloth](https://github.com/unslothai/unsloth)

[GPTQ by fpgaminer](https://github.com/fpgaminer/GPTQ-triton)

[GPTQ on PyTorch blog](https://pytorch.org/blog/accelerating-triton/)

[FlagAttention, memory-efficient attention kernels](https://github.com/FlagOpen/FlagAttention)

## Activations
[Activation functions by dogukantai](https://github.com/dogukantai/triton-activations)

## Matrix Operations
[Sparse Toolkit: Block-sparse matrix multiplication](https://github.com/stanford-futuredata/stk) ([paper](https://openreview.net/forum?id=doa11nN5vG))

[GemLite: Fused low-bit matrix multiplication](https://github.com/mobiusml/gemlite)

## Communication
[symm-mem-recipes](https://github.com/yifuwang/symm-mem-recipes)

[tccl](https://github.com/cchan/tccl)

## Quantization
[Quantization kernels by bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main/bitsandbytes/triton)

## Special operations
[EquiTriton for equivariant NN by IntelLabs](https://github.com/IntelLabs/EquiTriton)

## Benchmark
[TritonBench by PyTorch](https://github.com/pytorch-labs/tritonbench)

## Integrations
[JAX-Triton](https://github.com/jax-ml/jax-triton)

## Others
[Triton-distributed](https://github.com/ByteDance-Seed/Triton-distributed)
