# Awesome-Triton-Kernels
Collection of kernels written in Triton language (didn't seem to be a lot till now). Welcoming contribution!

[Main Repo by OpenAI](https://github.com/openai/triton)

[Official Tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html)

[Awesome resources from cuda-mode](https://github.com/cuda-mode/resource-stream), their [guide](https://www.youtube.com/watch?v=DdTsX6DQk24&ab_channel=CUDAMODE) to Triton

[Triton Kernel collection by cuda-mode](https://github.com/cuda-mode/triton-index)

[Puzzles by Sasha Rush](https://github.com/srush/Triton-Puzzles)

## General Operators
[attorch](https://github.com/BobMcDear/attorch) subset of PyTorch's nn module

[FlagGems](https://github.com/FlagOpen/FlagGems)

[Kernels by PyTorch Labs](https://github.com/pytorch-labs/applied-ai)

## Transformer
[Liger Kernel: Efficient Triton Kernels for LLM Training](https://github.com/linkedin/Liger-Kernel)

[FLASHNN for LLM Serving](https://github.com/AlibabaPAI/FLASHNN)

[Kernels by Kernl](https://github.com/ELS-RD/kernl)

[Kernels by Unsloth](https://github.com/unslothai/unsloth)

[GPTQ by fpgaminer](https://github.com/fpgaminer/GPTQ-triton)

[GPTQ on PyTorch blog](https://pytorch.org/blog/accelerating-triton/)

[FlagAttention, memory-efficient attention kernels](https://github.com/FlagOpen/FlagAttention)

## Activations
[Activation functions by dogukantai](https://github.com/dogukantai/triton-activations)

## Matrix Operations
[Sparse Toolkit: Block-sparse matrix multiplication](https://github.com/stanford-futuredata/stk) ([paper](https://openreview.net/forum?id=doa11nN5vG))

[GemLite: Fused low-bit matrix multiplication](https://github.com/mobiusml/gemlite)

## Integrations
[JAX-Triton](https://github.com/jax-ml/jax-triton)
